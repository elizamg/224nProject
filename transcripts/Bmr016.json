{
    "meeting_id": "Bmr016",
    "transcript": "Well , we actually figured out a way to the the groupings are randomly generated . I think they may still do it , um , And I well OK I it might help , I would like to g get away from having only one specific grouping . Um , so if that 's your question , but I mean it seems to me that , at least for us , we can learn to read them as digits if that 's what people want . I I 'm don't think that 'd be that hard to read them as single digits . Um , and it seems like that might be better for you guys since then you 'll have just more digit data , and that 's always a good thing . It 's a little bit better for me too because the digits are easier to recognize . They 're better trained than the numbers . Right . Right , read them as single digits , so sixty - one w is read as six one , and if people make a mistake we But it 's nice to get it in this room with the acous I mean for it 's Right . Well that 's fine with me as long as It 's just that I didn't want to cause the people who would have been collecting digits the other way to not have the digits . So OK . OK . It 's actually unclear right now . I just thought well we 're if we 're collec collecting digits , and Adam had said we were running out of the TI forms , I thought it 'd be nice to have them in groups , and probably , all else being equal , it 'd be better for me to just have single digits since it 's , you know , a recognizer 's gonna do better on those anyway , um , and it 's more predictable . So we can know from the transcript what the person said and the transcriber , in general . But if they make mistakes , it 's no big deal if the people say a hundred instead of \" one OO \" . and also w maybe we can just let them choose \" zero \" versus \" O \" as they as they like because even the same person c sometimes says \" O \" and sometimes says \" zero \" in different context , and that 's sort of interesting . So I don't have a Specific need cuz if I did I 'd probably try to collect it , you know , without bothering this group , but If we can try it Right , and you can give an example like , you know , \" six sixty - one would be read as six one \" . And I think people will get it . Right , right . It 's just easier to read . Right . OK - I also had a hard hard time with the words , but then we went back and forth on that . OK , so let 's give that a try and I mean what do other people think cuz you guys are reading them . OK . Great . OK . Well let 's give it a try . Righ - right , and you just they 're randomly generated and randomly assigned to digits . Right , just groupings in terms of number of groups in a line , and number of digits in a group , and the pattern of groupings . Um , I I just roughly looked at what kinds of digit strings are out there , and they 're usually grouped into either two , three , or four , four digits at a time . And they can have , I mean , actually , things are getting longer and longer . In the old days you probably only had three sequences , and telephone numbers were less , and so forth . So , there 's between , um Well if you look at it , there are between like three and five groups , and each one has between two and four groupings and I purposely didn't want them to look like they were in any kind of pattern . So Right . But I think it 'd be great i to be able to compare digits , whether it 's these digits or TI - digits , to speakers , um , and compare that to their spontaneous speech , and then we do need you know a fair amount of of digit data because you might be wearing a different microphone and , I mean so it 's it 's nice to have the digits you know , replicated many times . Especially for speakers that don't talk a lot . So um , for adaptation . No , I 'm serious , so we have a problem with acoustic adaptation , and we 're not using the digit data now , but you know Not for adaptation , nope . v W we 're not we were running adaptation only on the data that we ran recognition on and I 'd As soon as someone started to read transcript number , that 's read speech and I thought \" well , we 're gonna do better on that , that 's not fair to use \" . But , it might be fair to use the data for adaptation , so . So those speakers who are very quiet , shy r Right Well , it sh I mean it 's the same micropho see the nice thing is we have that in the in the same meeting , and so you don't get Right , and so I still like the idea of having some kind of digit data . The only thing that we don't have , I know this sounds weird , and maybe it 's completely stupid , but we don't have any overlapping digits . An - yea I know it 's weird , but um Alright everybody 's laughing . OK . I 'm just talkin for the stuff that like Dan Ellis is gonna try , you know , cross - talk cancellation . OK . Wait oh it these are all the same forms . OK So but So you plu you plug your ears . Well , what I mean is actually no not the overlaps that are well - governed linguistically , but the actual fact that there is speech coming from two people and the beam - forming stuf all the acoustic stuff that like Dan Ellis and and company want to do . Digits are nice and well behaved , I mean Anyway , it 's just a thought . It it would go faster . It would take one around amount of ti That 's right . I I mea I 'm I was sort of serious , but I really , I mean , I 'm I don't feel strongly enough that it 's a good idea , so . A and that prosody was great , by the way . It it sort of sounded like a duet , or something . OK . Five , six O six , five five I 'm sorry . I 'm mean I think it 's doable , I 'm just So , we we could have a round like where you do two at a time , and then the next person picks up when the first guy 's done , or something . Like a , what do you call it ? Li - a r like yeah , like that . Then it would go like h twice as fast , or a third as fast . Anyway , it 's just a thought . I 'm actually sort of serious if it would help people do that kind o but the people who wanna work on it we should talk to them . So . OK . Yeah . Can try it out . If we have nothing if we have no agenda we could do it some week . OK . I 'm all \" four \" it . Oh , oh ! Right , yeah . Well I was actually gonna skip the ASR results part , in favor of getting the transcription stuff talked about since I think that 's more important to moving forward , but I mean Morgan has this paper copy and if people have questions , um , it 's pretty preliminary in terms of ASR results because we didn't do anything fancy , but I think e just having the results there , and pointing out some main conclusions like it 's not the speaking style that differs , it 's the fact that there 's overlap that causes recognition errors . And then , the fact that it 's almost all insertion errors , which you would expect but you might also think that in the overlapped regions you would get substitutions and so forth , um , leads us to believe that doing a better segmentation , like your channel - based segmentation , or some kind of uh , echo cancellation to get basically back down to the individual speaker utterances would be probably all that we would need to be able to do good recognition on the on the close - talking mikes . So , that 's about the summary But this is Morgan has this paper . I mean he he it it 's that paper . Yeah , yeah . So , we basically , um , did a lot of work on that and it 's Let 's see , th I guess the other neat thing is it shows for sure w that the lapel , you know within speaker is bad . And it 's bad because it picks up the overlapping speech . Yes , cuz that 's all that w had been transcribed at the time , um but as we I mean I wanted to here more about the transcription . If we can get the channel asynchronous or the the closer t that would be very interesting for us because we Right . That 's cuz That 's right . In fact I I pulled out a couple classic examples in case you wanna u use them in your talk of Chuck on the lapel , so Chuck wore the lapel three out of four times . Um , yeah , and I wore the lapel once , and for me the lapel was OK . I mean I still and I don't know why . I 'm But um , for you it was Or who was next to me or something like that . Right , but when Chuck wore the lapel and Morgan was talking there 're a couple really long utterances where Chuck is saying a few things inside , and it 's picking up all of Morgan 's words pretty well and so the rec you know , there 're error rates because of insertion Insertions aren't bounded , so with a one - word utterance and ten insertions you know you got huge error rate . And that 's that 's where the problems come in . So I this is sort of what we expected , but it 's nice to be able to to show it . And also I just wanted to mention briefly that , um , uh Andreas and I called up Dan Ellis who 's still stuck in Switzerland , and we were gonna ask him if if there 're you know , what 's out there in terms of echo cancellation and things like that . Not that we were gonna do it , but we wanted to know what would need to be done . And he We 've given him the data we have so far , so these sychronous cases where there are overlap . And he 's gonna look into trying to run some things that are out there and see how well it can do because right now we 're not able to actually report on recognition in a real paper , like a Eurospeech paper , because it would look sort of premature . Right . Or who 's At any point in time who 's the foreground speaker , who 's the background speaker . So . So there 's like Well ther there 's Yeah . Yeah . Exactly , so it 's it 's a Um , it would be techniques used from adaptive adaptive echo cancellation which I don't know enough about to talk about . Um . But , right , um , and that would be similar to what you 're also trying to do , but using um , you know , more than energy I I don't know what exactly would go into it . So the idea is to basically run this on the whole meeting . and get the locations , which gives you also the time boundaries of the individual speak Right . Except that there are many techniques for the kinds of cues , um , that you can use to do that . So . And I guess Espen ? This is uh is he here too ? May also be working So it would just be ver that 's really the next step because we can't do too much , you know , on term in terms of recognition results knowing that this is a big problem um , until we can do that kind of processing . And so , once we have some some of yours , and @ @ we 'll move on . OK . Oh right . Yeah . Right . Definitely Uh , and Don should OK . Great . So that that 's it for the Speakers and OK . Great . Right . So at this point we can sort of finalize the naming , and so forth , and we 're gonna basically re rewrite out these waveforms that we did because as you notice in the paper your \" M O - four \" in one meeting and \" M O - two \" in another meeting and it 's we just need to standardize the um , no it 's it 's um , that 's why those comments are s are in there . So Right . OK . Great , great . Terrific . Eigh - eighteen . It was a spare that Dave had around Yeah it 's Well the Yeah . Well I 'd leave all the All the transcript stuff shouldn't should be backed up , but all the waveform Sound files should not be backed up , the ones that you write out . We can downsample them , so . Yeah . Yeah , we get the same performance . I mean the r the front - end on the SRI recognizer just downsamples them on the fly , so So that 's Yeah , if fe You 'd you wanna not . OK . So we 're what we 're doing is we 're writing out I mean , this is just a question . We 're writing out these individual segments , that wherever there 's a time boundary from Thilo , or or Jane 's transcribers , you know , we we chop it there . And the reason is so that we can feed it to the recognizer , and throw out ones that we 're not using and so forth . And those are the ones that we 're storing . So Yeah . Yeah . So we can't shorten them , but we can downsample them . So . Oh yeah th Yeah . Yeah . That that 's why we need more disk space cuz we 're basically duplicating the originals , um Oh yeah . No . We always have the original long ones . Um , it 's better if they 're chopped out , and and it it will be yeah , y we could probably write something to do that , but it 's actually convenient to have them chopped out cuz you can run them , you know , in different orders . You c you can actually move them around . Uh , you can get rid of Yeah , it it 's a lot faster . Right . You can grab everything with the word \" the \" in it , and it 's That 's a lot quicker than actually trying to access the wavefile each time , find the time boundaries and So in principle , yeah , you could do that , but it 's but it 's um These are long These are long You know . This is an hour of speech . We - yeah that 's so that 's part of it is Right . And the other part is just that once they 're written out it it is a lot faster to to process them . So . Otherwise , you 're just accessing Right . Right . The other thing is that , believe it or not I mean , we have some So we 're also looking at these in Waves like for the alignments and so forth . You can't load an hour of speech into X Waves . You need to s have these small files , and in fact , even for the Transcriber program Um Yeah , if you try to load s really long waveform into X Waves , you 'll be waiting there for Oh - Loading the long It takes a very long ti Right . It takes a l very long time . Huh . Actually , you 're talking about Transcriber , right ? Huh . Well we we have a problem with that , you know , time - wise on a It - it 's a lot slower to load in a long file , and also to check the file , so if you have a transcript , um , I mean it 's I I think overall you could get everything to work by accessing the same waveform and trying to find two you know , the begin and end times . Um , but I think it 's more efficient , if we have the storage space , to have the small ones . Yeah , it 's Yeah . It 's it 's just Right . Yeah , so these wouldn't be backed up , the Right . And Um , can I ask a question about the glossing , uh before we go on ? So , for a word like \" because \" is it that it 's always predictably \" because \" ? I mean , is \" CUZ \" always meaning \" because \" ? Beca - because Right . Right . Um , so , I guess So , from the point of view of The the only problem is that with for the recognition we we map it to \" because \" , and so if we know that \" CUZ \" but , we don't S Right . But , if it 's OK . But then there are other glosses that we don't replace , right ? Because OK . So , then it 's fine . OK . Yeah . Right . Actually we we gave this to our pronunciation person , she 's like , \" I don't know what that is either \" . So . No , we just gave her a list of words that , you know , weren't in our dictionary and so of course it picked up stuff like this , and she just didn't listen so she didn't know . We just we 're waiting on that just to do the alignments . Maybe it 's \" argh \" ? @ @ Yeah . Right , no one say Ah . But it has a different prosody . So , Jane , what 's the d I have one question about the the \" EH \" versus like the \" AH \" and the \" UH \" . @ @ OK . S OK . \" Eh , \" yeah right , cuz there were were some speakers that did definite \" eh 's \" but right now we So , it it 's actually probably good for us to know the difference between the real \" eh \" and the one that 's just like \" uh \" or transcribed \" aaa \" cuz in like in Switchboard , you would see e all of these forms , but they all were like \" uh \" . No , no , I mean like the the \" UH \" , or the \" UH \" , \" EH \" , \" AH \" were all the same . And then , we have this additional non - native version of uh , like \" eeh \" . Right . But you 're a native German speaker so it 's not a not a issue for It 's only Onl - yeah . No , only if you don't have lax vowels , I guess . Right . So it 's like Japanese and Spanish and Yeah this is great . This is really really helpful . Right . Right . Yeah I mean cuz they they have no idea , right . If you hear CTPD , I mean , they do pretty well but it 's you know how are how are they gonna know ? Yeah . Right . The recognizer , it is funny . Kept getting PTA for PDA . This is close , right , and the PTA was in these , uh , topics about children , so , anyway . Is the P - PTA working ? I 'm wai Well this is exactly how people will prove that these meetings do differ because we 're recording , right ? Y no normally you don't go around saying , \" Now you 've said it six times . Now you 've said \" Yeah . Well there 's not @ @ . Right . Huh . But at some point I mean , we probably shoul But we should add it to the dictionar No , to the pronunciation model . Language , uh Oh lan Oh OK - we OK - it 's in the language model , w yeah , but it so it 's the pronunciation model that has to have a pronunciation of \" tickle \" . I 'm sorry ! Oh , sorry . What I meant is that there should be a pronunciation \" tickle \" for TCL as a word . And that word in the in , you know , it stays in the language model wherever it was . Yeah you never would put \" tickle \" in the language model in that form , yeah . Right . There 's actually a bunch of cases like this with people 's names and Yes . Yeah . Yeah so th th there there 's a few cases like that where the um , the word needs to be spelled out in in a consistent way as it would appear in the language , but there 's not very many of these . Tcl 's one of them . Um , y yeah . Right . Right . We have this there is this thing I was gonna talk to you about at some point about , you know , what do we do with the dictionary as we 're up updating the dictionary , these changes have to be consistent with what 's in the Like spelling people 's names and so forth . If we make a spelling correction to their name , like someone had Deborah Tannen 's name mispelled , and since we know who that is , you know , we could correct it , but but we need to make sure we have the mispel If it doesn't get corrected we have to have a pronunciation as a mispelled word in the dictionary . Things like that . So . Right . Right . So if there 's things that get corrected before we get them , it 's it 's not an issue , but if there 's things that um , we change later , then we always have to keep our the dictionary up to date . And then , yeah , in the case of \" tickle \" I guess we would just have a , you know , word \" TCL \" which which normally would be an acronym , you know , \" TCL \" but just has another pronunciation . Oh yeah . Right , exactly . Right . Oh there is something spelled out \" BEEEEEEP \" in the old Thank you . Because he was saying , \" How many E 's do I have to allow for ? \" Right , thanks , yeah . Right . Right . Right . Right . Yeah . Yeah . Wait . Oh , so you could do \" grep minus V nums \" . So that 's the yeah . So there wouldn't be something like i if somebody said something like , \" Boy , I 'm really tired , OK . \" and then started reading that would be on a separate line ? OK great . Cuz I was doing the \" grep minus V \" quick and dirty and looked like that was working OK , but Great . Now why do we what 's the reason for having like the point five have the \" NUMS \" on it ? Is that just like when they 're talking about their data or something ? Or These are all like inside the spontaneous Oh OK . I see . OK . So , I guess I 'd have one request here which is just , um , maybe to make it more robust , th that the tag , whatever you would choose for this type of \" NUMS \" where it 's inside the spontaneous speech , is different than the tag that you use for the read speech . Um , that way w if we make a mistake parsing , or something , we don't see the \" point five \" , or or it 's not there , then we a Just an And actually for things like \" seven eighths \" , or people do fractions too I guess , you maybe you want one overall tag for sort of that would be similar to that , or As long as they 're sep as they 're different strings that we that 'll make our p sort of processing more robust . Cuz we really will get rid of everything that has the \" NUMS \" string in it . Yeah . It would probably be safer , if you 're willing , to have a separate tag just because um , then we know for sure . And we can also do counts on them without having to do the processing . But you 're right , we could do it this way , it it should work . Um , but it it 's probably not hard for a person to tell the difference because one 's in the context of a you know , a transcribed word string , and So \" Seven point five \" . The word . Yeah . And and actually , you know the language it 's the same point , actually , the the p you know , the word \" to \" and the word y th \" going to \" and \" to go to \" those are two different \" to 's \" and so there 's no distinction there . It 's just just the word \" point \" has Yeah , every word has only one , yeah e one version even if even if it 's A actually even like the word \" read \" and \" read \" . Those are two different words . They 're spelled the same way , right ? And they 're still gonna be transcribed as READ . So , yeah , I I like the idea of having this in there , I just I was a little bit worried that , um , the tag for removing the read speech because i What if we have like \" read letters \" or , I don't know , like \" read something \" like \" read \" yeah , basically . But other than that I it sounds great . But that that 's not hard . I I think the harder part is making sure that the transc the transcription So if you b merge two things , then you know that it 's the sum of the transcripts , but if you split inside something , you don't where the word which words moved . And that 's wh that 's where it becomes a little bit uh , having to rerun the processing . The cutting of the waveforms is pretty trivial . Right ."
}