{
    "meeting_id": "Bro004",
    "transcript": "Channel channel three , yeah . OK . Mike four . Uh so . uh We So As I was already said , we we mainly focused on uh four kind of features . The PLP , the PLP with JRASTA , the MSG , and the MFCC from the baseline Aurora . Uh , and we focused for the the test part on the English and the Italian . Um . We 've trained uh several neural networks on so on the TI - digits English and on the Italian data and also on the broad uh English uh French and uh Spanish databases . Mmm , so there 's our result tables here , for the tandem approach , and um , actually what we we @ @ observed is that if the network is trained on the task data it works pretty well . Yeah , so if the network is trained on the task data um tandem works pretty well . And uh actually we have uh , results are similar Only on , yeah . Just that task . But actually we didn't train network on uh both types of data I mean uh phonetically ba phonetically balanced uh data and task data . We only did either task task data or uh broad data . Um Yeah . So , Mmm . Yeah . If we use the same language ? Mm - hmm . But Yeah but I did not uh do that . We No , we did four four kind of of testing , actually . The first testing is with task data So , with nets trained on task data . So for Italian on the Italian speech @ @ . The second test is trained on a single language um with broad database , but the same language as the t task data . But for Italian we choose Spanish which we assume is close to Italian . The third test is by using , um the three language database and the fourth is This includes Yeah . But not digits . I mean it 's Yeah And the fourth test is uh excluding from these three languages the language that is the task language . Yeah . Uh , yeah . So um for uh TI - digits for ins example uh when we go from TI - digits training to TIMIT training uh we lose uh around ten percent , uh . The error rate increase u of of of ten percent , relative . So this is not so bad . And then when we jump to the multilingual data it 's uh it become worse and , well Around uh , let 's say , twenty perc twenty percent further . So . Yeah . Twenty to to thirty percent further . Yeah . Yeah . Yeah . Yeah . But the first step is al already removing the task s specific from from So . And we lose Yeah . Uh So , basically when it 's trained on the the multilingual broad data um or number so , the the ratio of our error rates uh with the baseline error rate is around uh one point one . So . No no no . Uh same language we are at uh for at English at O point eight . So it improves , compared to the baseline . But So . Le - let me . Tas - task data we are u Yeah . Mmm . Hmm . Oh yeah , the f Yeah , OK . Yeah . Yeah . Sure . Mmm . Mm - hmm . Mm - hmm . Mm - hmm . Yeah , it 's around one point one . Yeah . Ye - Uh , more actually . If I Yeah . What would you say ? Around one point four yeah . If we exclude English , um there is not much difference with the data with English . So . Yeah . Uh . Hmm . Yeah . The only difference it 's is that it 's multilingual Um Yeah . Yeah sure . Uh yeah . A part of it , yeah . Um It 's two times , actually ? Yeah . Um . The English data No , the multilingual databases are two times the broad English data . We just wanted to keep this , w well , not too huge . So . I think so . Do you Uh , Yeah . Yeah . Mm - hmm . Yeah . Hmm ? Yep . Mmm . Uh , let me check . Uh . So . This was for the PLP , Um . The Yeah . For the PLP with JRASTA the the we This is quite the same tendency , with a slight increase of the error rate , uh if we go to to TIMIT . And then it 's it gets worse with the multilingual . Um . Yeah . There there is a difference actually with b between PLP and JRASTA is that JRASTA seems to perform better with the highly mismatched condition but slightly slightly worse for the well matched condition . Mmm . Yeah , yeah . OK . Uh , no , no . Training on a single language , you mean , and testing on the other one ? Uh , no . So the only task that 's similar to this is the training on two languages , and that Uh , No . Either thi this is test with uh the same language but from the broad data , or it 's test with uh different languages also from the broad data , excluding the So , it 's it 's three or three and four . Uh . No . You mean training digits on one language and using the net to recognize on the other ? No . Uh , No , I don't think so . So . So you have uh basically two uh parts . The upper part is for TI - digits and it 's divided in three rows of four four rows each . And the first four rows is well - matched , then the s the second group of four rows is mismatched , and finally highly mismatched . And then the lower part is for Italian and it 's the same the same thing . So . It 's it 's the HTK results , I mean . So it 's HTK training testings with different kind of features and what appears in the uh left column is the networks that are used for doing this . So . Uh Yeah . It - It was part of these results . Mmm . Mmm . You mean the HTK Aurora baseline ? It 's uh the one hundred number . It 's , well , all these numbers are the ratio with respect to the baseline . Yeah , this is a word error rate ratio . Yeah . So , seventy point two means that we reduced the error rate uh by thirty thirty percent . So . Hmm . Yeah . To TIMIT . Mmm . Then you have uh MF , MS and ME which are for French , Spanish and English . And , yeah . Actually I I uh forgot to say that the multilingual net are trained on uh features without the s derivatives uh but with increased frame numbers . Mmm . And we can we can see on the first line of the table that it it it 's slightly slightly worse when we don't use delta but it 's not not that much . So . Multi - French , Multi - Spanish , and Multi - English . Yeah . Yeah . Yeah . Still poor . Yeah . Yeah . No these are the s s s same noises , yeah . At least at least for the first for the well - matched , yeah . Mmm . Yeah . Yeah , so for the Italian the results are uh stranger um Mmm . So what appears is that perhaps Spanish is not very close to Italian because uh , well , when using the the network trained only on Spanish it 's the error rate is almost uh twice the baseline error rate . Mmm . Uh . Yeah . There there is another difference , is that the noise the noises are different . Well , For for the Italian part I mean the uh the um networks are trained with noise from Aurora TI - digits , mmm . Yeah . And perhaps the noise are quite different from the noises in the speech that Italian . And No . Yeah . Yeah . It 's no , the third part , so it 's uh highly mismatched . So . Training and test noise are different . Yeah . Yeah . Yeah . Yeah . But it 's not a clean case . It 's a noisy case but uh training and test noises are the same . So Yeah . Yeah . So it 's always noisy basically , and , well , the Mmm . Uh , no we don't plan to fill the holes but actually there is something important , is that um we made a lot of assumption concerning the on - line normalization and we just noticed uh recently that uh the approach that we were using was not uh leading to very good results when we used the straight features to HTK . Um Mmm . So basically d if you look at the at the left of the table , the first uh row , with eighty - six , one hundred , and forty - three and seventy - five , these are the results we obtained for Italian uh with straight mmm , PLP features using on - line normalization . Mmm . And the , mmm what 's in the table , just at the left of the PLP twelve on - line normalization column , so , the numbers seventy - nine , fifty - four and uh forty - two are the results obtained by uh Pratibha with uh his on - line normalization uh her on - line normalization approach . So . Just uh Yeah . So these are the results of OGI with on - line normalization and straight features to HTK . And the previous result , eighty - six and so on , are with our features straight to HTK . So what we see that is there is that um uh the way we were doing this was not correct , but still the networks are very good . When we use the networks our number are better that uh Pratibha results . Yeah . There were diff there were different things and basically , the first thing is the mmm , alpha uh value . So , the recursion uh part . um , I used point five percent , which was the default value in the in the programs here . And Pratibha used five percent . So it adapts more quickly Um , but , yeah . I assume that this was not important because uh previous results from from Dan and show that basically the both both values g give the same same uh results . It was true on uh TI - digits but it 's not true on Italian . Uh , second thing is the initialization of the stuff . Actually , uh what we were doing is to start the recursion from the beginning of the utterance . And using initial values that are the global mean and variances measured across the whole database . And Pratibha did something different is that he uh she initialed the um values of the mean and variance by computing this on the twenty - five first frames of each utterance . Mmm . There were other minor differences , the fact that she used fifteen dissities instead s instead of thirteen , and that she used C - zero instead of log energy . Uh , but the main differences concerns the recursion . So . Uh , I changed the code uh and now we have a baseline that 's similar to the OGI baseline . We It it 's slightly uh different because I don't exactly initialize the same way she does . Actually I start , mmm , I don't wait to a fifteen twenty - five twenty - five frames before computing a mean and the variance to e to to start the recursion . I I use the on - line scheme and only start the re recursion after the twenty - five twenty - fifth frame . But , well it 's similar . So uh I retrained the networks with these well , the the the networks are retaining with these new features . And , yeah . So basically what I expect is that these numbers will a little bit go down but perhaps not not so much because I think the neural networks learn perhaps to even if the features are not normalized . It it will learn how to normalize and Mmm . Yeah . Yeah I 'd No , I we plan to start this uh so , act actually we have discussed uh @ @ um , these what we could do more as a as a research and and we were thinking perhaps that uh the way we use the tandem is not Uh , well , there is basically perhaps a flaw in the in the the stuff because we trained the networks If we trained the networks on the on a language and a t or a specific task , um , what we ask is to the network is to put the bound the decision boundaries somewhere in the space . And uh mmm and ask the network to put one , at one side of the for for a particular phoneme at one side of the boundary decision boundary and one for another phoneme at the other side . And so there is kind of reduction of the information there that 's not correct because if we change task and if the phonemes are not in the same context in the new task , obviously the decision boundaries are not should not be at the same place . But the way the feature gives The the way the network gives the features is that it reduce completely the it removes completely the information a lot of information from the the features by uh uh placing the decision boundaries at optimal places for one kind of data but this is not the case for another kind of data . So Yeah . So uh what we were thinking about is perhaps um one way to solve this problem is increase the number of outputs of the neural networks . Doing something like , um um phonemes within context and , well , basically context dependent phonemes . Yeah but , we know that Ye - yeah but here it 's something different . We want to have features uh well , um . Mm - hmm . Yeah . Yeah , but mmm , I mean , the the way we we do it now is that we have a neural network and basically the net network is trained almost to give binary decisions . And uh binary decisions about phonemes . Nnn Uh It 's Yeah . Yeah . Yeah , sure but uh So basically it 's almost binary decisions and um the idea of using more classes is to get something that 's less binary decisions . But yeah , but Yeah , but if Mmm . Mm - hmm . Mmm . Yeah , but I think Yeah , perhaps you 're right , but you have more classes so you you have more information in your features . So , Um You have more information in the uh posteriors vector um which means that But still the information is relevant because it 's it 's information that helps to discriminate , if it 's possible to be able to discriminate among the phonemes in context . But the Mmm . Mmm . Mmm . Mmm . Mm - hmm . Mm - hmm . N Yeah . Yeah . Uh so there is this combination , yeah . Working on combination obviously . Um , I will start work on multi - band . And we plan to work also on the idea of using both features and net outputs . Um . And we think that with this approach perhaps we could reduce the number of outputs of the neural network . Um , So , get simpler networks , because we still have the features . So we have um come up with um different kind of broad phonetic categories . And we have Basically we have three types of broad phonetic classes . Well , something using place of articulation which which leads to nine , I think , broad classes . Uh , another which is based on manner , which is is also something like nine classes . And then , something that combine both , and we have twenty f twenty - five ? Twenty - seven broad classes . So like , uh , oh , I don't know , like back vowels , front vowels . Um For the moments we do not don't have nets , I mean , It 's just Were we just changing the labels to retrain nets with fewer out outputs . And then Mm - hmm . It - It 's the single net , yeah . It 's one net with um twenty - seven outputs if we have twenty - seven classes , yeah . So it 's Well , it 's basically a standard net with fewer classes . Yeah , but I think Yeah . B b including the features , yeah . I don't think this will work alone . I think it will get worse because Well , I believe the effect that of of too reducing too much the information is basically basically what happens and but Yeah , because there is perhaps one important thing that the net brings , and OGI show showed that , is the distinction between sp speech and silence Because these nets are trained on well - controlled condition . I mean the labels are obtained on clean speech , and we add noise after . So this is one thing And But perhaps , something intermediary using also some broad classes could could bring so much more information . Uh . Yeah . Yeah . Mm - hmm . There will probably be , yeah , one single KL to transform everything or uh , per This is still something that yeah , we don't know Yeah . Yeah . Mmm . Uh , yeah . Mm - hmm . Mm - hmm . Uh , no . I don't think so . Yeah , I have one . Mm - hmm . Mm - hmm . Yeah . Mmm . Mmm . Mmm . The features , yeah . Yeah . Yeah , I don't know . I think I think it 's the C - zero using C - zero instead of log energy . Yeah , it 's this . It should be that , yeah . Because Yeah . i Yeah . Yeah . Yeah , you can basically remove the the frames from the feature feature files . And . I t Mm - hmm . Yeah . I think we 're alright , um , not much problems with that . It 's OK . Well this table took uh more than five days to get back . But Yeah . Mmm , no . You were using Gin perhaps , yeah ? No . Hmm . Mm - hmm . Mmm . Yeah . Yeah . For HTK ? Uh Training is longer . Yeah . Mmm . Mmm . Yeah . Transcript two zero three one dash two zero five zero zero two one two O two six six three two seven three four nine seven nine O five six O O two eight one two nine four seven nine one six five O eight three four zero five three one two O six O seven three zero five two six four eight eight one seven eight six seven four eight six one nine four nine O"
}