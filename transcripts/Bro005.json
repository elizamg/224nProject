{
    "meeting_id": "Bro005",
    "transcript": "Sorry , I 'm very late , uh , Am I still accommodated , or ? Would you be using on - line normalization with that ? Would you be using on - line normalization with LogRASTA - PLP ? Yeah . Um . It also seems like whe if you try to train , like , a single MLP with too much noise , um , you 'll get some nice interval of power for unseen cases but um but any unmatched cases it 'll start , um , interfering . Well , it 's easier . Um , that way you can turn things off @ @ turn things on . Um . But you then A single MLP of course will work a little bit better because uh it 'll have uh It 's a nonlinear kind of merging of the features . Uh , well , in general nonlinear mergings seems to work a little better then just the straight linear coefficients , but um it just means it 's you have to have bigger nets and more training time , and if you want to turn things off um that 's harder to do . I think the way that Hynek or or Malik had had told me was that if you try to train a classifier on too much too many conditions then it 'll do good on none of them , but it 'll start doing something else . Which means , if you have something else in there it 'll be nicer @ @ I 'm not sure @ @ I also have some some a new theory on why um LogRASTA - PLP uh PLP with on - line normalization might be a little bit better than LogRASTA - PLP with on - line normalization . It has to do with certain distribution characteristics . But if you take away the on - line normalization , LogRASTA seems to do better than PLP but not in all cases . Um . not so much not right , but it if you throw in the on - line normalization then it might not be necessary to use the LogRASTA - PLP . @ @ . Light 's on here . @ @ Yeah ."
}