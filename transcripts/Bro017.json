{
    "meeting_id": "Bro017",
    "transcript": "Yep . That 's right . Yeah . Yeah , that 's usually what we do . We just sorta go around and people say what 's going on , what 's the latest uh Yeah . That would be great . Yeah . It seemed like there were still some issues , right ? that they were trying to decide ? Like the voice activity detector , and Right . Right . Right . Right . Mm - hmm . Right . Yeah . Right . Right . Mm - hmm . Yeah . Mm - hmm . Right . Right . Mm - hmm . Ah . Hmm . Right . But wait a minute , I thought the endpointing really only helped in the noisy cases . Oh , but you still have that with the MFCC . OK . Yeah . Right . Yeah . Yeah . Yeah . Hmm . Mm - hmm . Mm - hmm . Mm - hmm . Hmm . So what about the issue of um the weights on the for the different systems , the well - matched , and medium - mismatched and Mm - hmm . Mm - hmm . And they 're the staying the same ? Mm - hmm . Right . Yeah . Mm - hmm . Mm - hmm . Hmm . Hmm . You haven't tried that yet ? Hmm . Hmm ! Oh ! I see . Right . Well you 're you 're basically y Yeah . So you 're making all your training data more uniform . Hmm . Mm - hmm . Hmm . Hmm . Hmm ! What is that ? The TRAPS sound familiar , I but I don't Mm - hmm . Mm - hmm . Mm - hmm . Mm - hmm . Mm - hmm . Hmm . Hmm . But you have many of those vectors per phoneme , right ? Uh - huh . Hmm . Hmm . Hmm . How wide are the uh frequency bands ? Well how do you how do you uh convert this uh energy over time in a particular frequency band into a vector of numbers ? Yeah but what 's the number ? Is it just the it 's just the amount of energy in that band from f in that time interval . OK . Mm - hmm . Yeah . Yeah . Mm - hmm . Hmm . Hmm . Mm - hmm . Hmm . So it 's really w It 's sort of like saying that what 's happening at one kilohertz depends on what 's happening around it . It 's sort of relative to it . Mm - hmm . Hmm . Hey Stephane . Yep . Sure . Go ahead . Mm - hmm . Hmm . Mmm . Mmm . Yeah . Mm - hmm . Yeah . Mm - hmm . But you probably won't have anything before the next time we have to evaluate , right ? Yeah . Ah . Latency and things . Mm - hmm . Yeah . Mm - hmm . Hmm . Wow . Is he getting married or something ? Right . Well , I mean , I 've known other friends who they they go to Ind - they go back home to India for a month , they come back married , you know , huh . So he finally had some incentive to finish , huh ? Oh . When he knew you were happy , huh ? Hmm . Hmm . So have the um when is the next uh evaluation ? June or something ? No , for uh Aurora ? Hmm . Oh , OK . Are people supposed to rerun their systems , or ? Hmm . Wow . Mm - hmm . Hmm . Do you know what the new baseline is ? Oh , I guess if you don't have Using your uh voice activity detector ? Similar , yeah . Mm - hmm . Yeah . Yeah . Oh , OK . Hmm . Hmm . Wow . Where 's uh Guenter going ? Mm - hmm . Hmm ! Mm - hmm . Mm - hmm . Hmm ! Mm - hmm . Wow ! Hmm . Hmm . How 's your documentation or whatever it w what was it you guys were working on last week ? Hmm . So have you been running some new experiments ? I I thought I saw some jobs of yours running on some of the machine Really ? ! That has no effect ? Eh Is this in the baseline ? or in uh in uh - huh , uh - huh . In some what ? Mm - hmm . Mm - hmm . Mm - hmm . You 're compressing the range , right ? of that Mm - hmm . Oh . Yeah . Uh - huh . Right . Mm - hmm . Mm - hmm . Hmm . Insertions . Oh . Uh - huh . You need like a some kind of a You need to have a genetic algorithm , that basically tries random permutations of these things . Yeah . Mm - hmm . Yeah . You know actually , I don't know that this wouldn't be all that bad . I mean you you compute the features once , right ? And then these exponents are just applied to that So . And is this something that you would adjust for training ? or only recognition ? You would do it on both . So you 'd actually Mm - hmm . So for each uh set of exponents that you would try , it would require a training and a recognition ? Oh . So if you Instead of altering the feature vectors themselves , you you modify the the the Gaussians in the models . Uh - huh . But why if you 're if you 're multi if you 're altering the model , why w in the test data , why would you have to muck with the uh cepstral coefficients ? No . But you 're running your data through that same model . Mm - hmm . Mm - hmm . Mm - hmm . All of the all of the mean and variances that correspond to C - one , you put them to zero . Yeah . But what are you multiplying ? Cuz those are means , right ? I mean you 're I think you Yeah , I think you 'd have to modify the standard deviation or something , so that you make it wider or narrower . Oop . Sorry . So . So by making th the standard deviation narrower , uh your scores get worse for unless it 's exactly right on the mean . Right ? I mean there 's you 're you 're allowing for less variance . Mm - hmm . Mm - hmm . Mm - hmm . Couldn't you just do that to the test data and not do anything with your training data ? Uh - huh . Mm - hmm . Could you also if you wanted to if you wanted to try an experiment uh by leaving out say , C - one , couldn't you , in your test data , uh modify the all of the C - one values to be um way outside of the normal range of the Gaussian for C - one that was trained in the model ? So that effectively , the C - one never really contributes to the score ? Do you know what I 'm say Yeah , someth Mm - hmm . But what if you set if to the mean of the model , then ? And it was a cons you set all C - ones coming in through your test data , you you change whatever value that was there to the mean that your model had . Yeah . Oh , that 's true , right , yeah , because you you have Yeah . Mm - hmm . Mm - hmm . Mm - hmm . Mm - hmm . Mm - hmm . Mm - hmm . You 're talking about the standard deviation ? Yeah . Mm - hmm . But don't you have to do something to the mean , also ? But if your If your um original data for C - one had a mean of two . And now you 're you 're you 're changing that by squaring it . Now your mean of your C - one original data has is four . But your model still has a mean of two . So even though you 've expended the range , your mean doesn't match anymore . Do you see what I mean ? Mm - hmm . Yeah . Right . Yeah , so y It 's predictable , yeah . Yeah . But as a simple thing , you could just just muck with the variance . to get uh this uh this the effect I think that you 're talking about , right ? Could increase the variance to decrease the importance . Yeah , because if you had a huge variance , you 're dividing by a large number , you get a very small contribution . Yeah . Hmm . Yeah , you know actually , this reminds me of something that happened uh when I was at BBN . We were playing with putting um pitch into the Mandarin recognizer . And this particular pitch algorithm um when it didn't think there was any voicing , was spitting out zeros . So we were getting uh when we did clustering , we were getting groups uh of features yeah , with with a mean of zero and basically zero variance . So , when ener when anytime any one of those vectors came in that had a zero in it , we got a great score . I mean it was just , you know , incredibly high score , and so that was throwing everything off . So if you have very small variance you get really good scores when you get something that matches . So . So that 's a way , yeah , yeah That 's a way to increase the yeah , n That 's interesting . So in fact , that would be That doesn't require any retraining . So that means it 's just recognitions . Yeah . You you have a step where you you modify the models , make a d copy of your models with whatever variance modifications you make , and rerun recognition . And then do a whole bunch of those . That could be set up fairly easily I think , and you have a whole bunch of you know That 's an interesting idea , actually . For testing the Yeah . Huh ! That 's right . In fact , and and they 're just t right now they 're installing uh increasing the memory on that uh the Linux box . Yeah . Absinthe . Absinthe . We 've got five processors on that . And two gigs of memory . Yeah . Exactly . Yeah . See how many cycles we used ? Yeah . Uh - huh . Oh , my gosh ! So he had to make it look like Yeah . How Idle time . Yeah . Have you ever seen those little um It 's it 's this thing that 's the shape of a bird and it has a red ball and its beak dips into the water ? So if you could hook that up so it hit the keyboard That 's an interesting experiment . Mm - hmm . Mm - hmm . Ugh ! Yeah . Yeah . Yeah . And there wasn't ? Huh ! Well that 's that 's a really i That wouldn't be too difficult to try . Maybe I could set that up . And we 'll just Yeah , so the first set of uh variance weighting vectors would be just you know one modifying one and leaving the others the same . And and do that for each one . That would be one set of experiment Wh - yeah , when the data matches that , then you get really Yeah . Right . But there could just naturally be low variance . Because I Like , I 've noticed in the higher cepstral coefficients , the numbers seem to get smaller , right ? So d I mean , just naturally . Yeah . Exactly . And so it seems like they 're already sort of compressed . The range of values . Mm - hmm . Mm - hmm . Hmm . Mm - hmm . Hmm . Mm - hmm . Hmm . Any Is the um if we mail to \" Aurora - inhouse \" , does that go up to you guys also ? OK . So i What is it Yeah we sh Do we have a mailing list that includes uh the OGI people ? Oh ! Maybe we should set that up . That would make it much easier . So maybe just call it \" Aurora \" or something that would Mm - hmm . Mm - hmm . OK . Maybe we can set that up . Mm - hmm . Hmm ! Mm - hmm . Hmm . Hmm . Mm - hmm . Wow ! Hmm . Hmm . Yeah . Wow ! Hmm . Does anybody have anything else ? to Shall we read some digits ? So . Hynek , I don't know if you 've ever done this . The way that it works is each person goes around in turn , and uh you say the transcript number and then you read the digits , the the strings of numbers as individual digits . So you don't say \" eight hundred and fifty \" , you say \" eight five oh \" , and so forth . Um . Sure . Transcript L one zero two . two nine five six four five two six seven six eight four three eight four nine one three four eight nine zero two five six four five two two three two nine two nine nine zero one three one seven eight zero nine two nine three six six zero zero five six seven four two one three three seven six six eight three three three two one zero three two six six nine one three seven nine nine one"
}